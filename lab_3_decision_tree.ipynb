{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from IPython.core.display import display\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import math\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import pprint\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encodings(enc):\n",
    "    encoding = enc.categories_\n",
    "    encoding_feature = lambda x: dict(zip(x, range(len(x))))\n",
    "    encoding_full = [encoding_feature(feature_elem) for feature_elem in encoding]\n",
    "    pp.pprint(encoding_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
    "### Code Requirements/Notes:\n",
    "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits. Otherwise, even with approaches like pruning below, the SSE type of overfit could still hurt us.) \n",
    "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
    "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode():\n",
    "    def __init__(self, X_data, Y_data, depth=1, is_split_node=True,\n",
    "                 split_feature_index=0, feature_attribute_index=0, parent=None):\n",
    "\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.depth = depth\n",
    "        self.is_split_node = is_split_node # Is this node a split node?\n",
    "\n",
    "        self.split_feature_index = split_feature_index # The index of the input features that node will split on\n",
    "        self.feature_attribute_index = feature_attribute_index\n",
    "\n",
    "        self.parent = parent # Pointer back to parent node. For computing gain\n",
    "\n",
    "        self.value = [] # The values of each output class represented in the node\n",
    "        self.children = [] # The child nodes of the self node after deciding split\n",
    "\n",
    "        self.pred_out_class = -1\n",
    "            # print('\\t'*self.depth, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self,counts=None, max_depth=np.inf):\n",
    "        \"\"\" Initialize class with chosen hyperparameters.\n",
    "        Args:\n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            counts: A list of Ints that tell you how many types of each feature there are\n",
    "        Example:\n",
    "            DT  = DTClassifier()\n",
    "            or\n",
    "            DT = DTClassifier(count = [2,3,2,2])\n",
    "            Dataset = \n",
    "            [[0,1,0,0],\n",
    "            [1,2,1,1],\n",
    "            [0,1,1,0],\n",
    "            [1,2,0,1],\n",
    "            [0,0,1,1]]\n",
    "\n",
    "        \"\"\"\n",
    "        self.counts = counts\n",
    "        self.root = None\n",
    "        self.n_output_classes = None\n",
    "        self.split_information_gains = []\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data; Make the Decision tree\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "            y (array-like): A 1D numpy array with the training targets\n",
    "\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "\n",
    "        \"\"\"\n",
    "        self.root = DTNode(X, y)\n",
    "        self.n_output_classes = len(np.unique(y))\n",
    "\n",
    "        self._make_split(self.root)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict all classes for a dataset X\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = np.zeros(shape=(X.shape[0]))\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            instance = X[i, :]\n",
    "            prediction = self._predict(self.root, instance)\n",
    "            predictions[i] = prediction\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _predict(self, node, instance):\n",
    "\n",
    "        if not node.is_split_node:\n",
    "            return node.pred_out_class\n",
    "\n",
    "        split_feature_index = node.split_feature_index\n",
    "        instance_attribute_val = int(instance[split_feature_index])\n",
    "        next_node = node.children[instance_attribute_val]\n",
    "        return self._predict(next_node, instance)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 1D numpy array of the targets \n",
    "        \"\"\"\n",
    "\n",
    "        predictions = self.predict(X)\n",
    "\n",
    "        score = np.mean(np.where(y == predictions, 1, 0))\n",
    "\n",
    "        return score\n",
    "\n",
    "    def print_tree(self, feature_names, enc):\n",
    "        self.print_tree_helper(self.root, feature_names, enc)\n",
    "\n",
    "    def print_tree_helper(self, node, feature_names, enc):\n",
    "        if node.is_split_node:\n",
    "\n",
    "            for i in range(len(node.children)):\n",
    "                child_node = node.children[i]\n",
    "                print('\\t'*(node.depth-1),\n",
    "                      feature_names[node.split_feature_index], ' = ',\n",
    "                      enc.categories_[node.split_feature_index][i], \":\")\n",
    "                self.print_tree_helper(child_node, feature_names, enc)\n",
    "\n",
    "        else:\n",
    "            print('\\t'*(node.depth-1), \"prediction: \", enc.categories_[-1][node.pred_out_class])\n",
    "\n",
    "    def _info(self, node):\n",
    "        X = node.X_data\n",
    "        Y = node.Y_data\n",
    "\n",
    "        node_info = 0\n",
    "        for C in range(self.n_output_classes): # For each output class\n",
    "            filter = np.where(Y == C, 1, 0)\n",
    "            n_instances = np.sum(filter)\n",
    "\n",
    "            if len(Y) == 0:\n",
    "                return 0\n",
    "\n",
    "            p_i = np.sum(filter) / len(Y)\n",
    "\n",
    "            if len(Y) == 0:\n",
    "                return 0\n",
    "\n",
    "            if p_i != 0:\n",
    "                node_info += p_i * np.log2(p_i)\n",
    "\n",
    "        return -node_info\n",
    "\n",
    "    def _make_split(self, node):\n",
    "        # Base case - When there is only one class in node\n",
    "        if (len(np.unique(node.Y_data)) == 1 or \\\n",
    "                node.depth == self.max_depth):\n",
    "            node.is_split_node = False\n",
    "            node.pred_out_class = int(stats.mode(node.Y_data)[0])\n",
    "            return\n",
    "\n",
    "        # Get the best split children and feature split index\n",
    "        node.children, node.split_feature_index = \\\n",
    "            self._get_best_split_children(node)\n",
    "\n",
    "        # If we have run out of attributes to split on\n",
    "        if None in node.children:\n",
    "            node.children = []\n",
    "            node.is_split_node = False\n",
    "            node.pred_out_class = int(stats.mode(node.Y_data)[0])\n",
    "            return\n",
    "\n",
    "        # Split on each child node\n",
    "        for child_node in node.children:\n",
    "            self._make_split(child_node)\n",
    "\n",
    "    def _get_best_split_children(self, node):\n",
    "\n",
    "        # For each input feature\n",
    "        X = node.X_data\n",
    "        Y = node.Y_data\n",
    "        len_S = len(Y)\n",
    "\n",
    "        max_gain = 0\n",
    "        best_feature_split_ind = 0\n",
    "        best_split_children = None\n",
    "\n",
    "        for feature_ind in range(X.shape[1]):\n",
    "\n",
    "            # Skip if the attribute only has one unique value\n",
    "            if (len(np.unique(X[:, feature_ind])) == 1):\n",
    "                continue\n",
    "\n",
    "            # Calculate the gain of splitting on that feature\n",
    "            contender_child_nodes = []\n",
    "            info_A = 0\n",
    "\n",
    "            for attribute_val in range(self.counts[feature_ind]):\n",
    "\n",
    "                mask = np.where(X[:, feature_ind] == attribute_val)\n",
    "\n",
    "                X_new = X[mask]\n",
    "                Y_new = Y[mask]\n",
    "\n",
    "                if len(Y_new) == 0:\n",
    "                    contender_child_nodes.append(None)\n",
    "                    continue\n",
    "\n",
    "                node_new = DTNode(X_new, Y_new, depth=node.depth + 1,\n",
    "                                  is_split_node=True, split_feature_index=feature_ind,\n",
    "                                  feature_attribute_index=attribute_val,\n",
    "                                  parent=node)\n",
    "                contender_child_nodes.append(node_new)\n",
    "\n",
    "                info_S_j = self._info(node_new)\n",
    "                if info_S_j == None:\n",
    "                    info_S_j = 0\n",
    "                len_S_j = len(Y_new)\n",
    "\n",
    "                info_A += (len_S_j / len_S) * info_S_j\n",
    "\n",
    "            gain = self._info(node) - info_A\n",
    "\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                best_feature_split_ind = feature_ind\n",
    "                best_split_children = contender_child_nodes\n",
    "\n",
    "        self.split_information_gains.append(max_gain)\n",
    "        return best_split_children, best_feature_split_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Debug\n",
    "\n",
    "Debug your model by training on the lenses dataset: [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses.arff)\n",
    "\n",
    "Test your model on the lenses test set: [Debug Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/lenses_test.arff)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "\n",
    "Expected Results: Accuracy = [0.33]\n",
    "\n",
    "Predictions should match this file: [Lenses Predictions](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv)\n",
    "\n",
    "*NOTE: The [Lenses Prediction](https://raw.githubusercontent.com/cs472ta/CS472/master/debug_solutions/pred_lenses.csv) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
    "\n",
    "Split Information Gains (These do not need to be in this exact order):\n",
    "\n",
    "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
    "\n",
    "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
    "\n",
    "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
    "\n",
    "Decision Tree:\n",
    "<pre>\n",
    "tear_prod_rate = normal:\n",
    "    astigmatism = no:\n",
    "        age = pre_presbyopic:\n",
    "            prediction: soft\n",
    "        age = presbyopic:\n",
    "            spectacle_prescrip = hypermetrope:\n",
    "                prediction: soft\n",
    "            spectacle_prescrip = myope:\n",
    "                prediction: none\n",
    "        age = young:\n",
    "            prediction: soft\n",
    "    astigmatism = yes:\n",
    "        spectacle_prescrip = hypermetrope:\n",
    "            age = pre_presbyopic:\n",
    "                prediction: none\n",
    "            age = presbyopic:\n",
    "                prediction: none\n",
    "            age = young:\n",
    "                prediction: hard\n",
    "        spectacle_prescrip = myope:\n",
    "            prediction: hard\n",
    "tear_prod_rate = reduced:\n",
    "    prediction: none\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      " tear_prod_rate  =  normal :\n",
      "\t astigmatism  =  no :\n",
      "\t\t age  =  pre_presbyopic :\n",
      "\t\t\t prediction:  soft\n",
      "\t\t age  =  presbyopic :\n",
      "\t\t\t spectacle_prescrip  =  hypermetrope :\n",
      "\t\t\t\t prediction:  soft\n",
      "\t\t\t spectacle_prescrip  =  myope :\n",
      "\t\t\t\t prediction:  none\n",
      "\t\t age  =  young :\n",
      "\t\t\t prediction:  soft\n",
      "\t astigmatism  =  yes :\n",
      "\t\t spectacle_prescrip  =  hypermetrope :\n",
      "\t\t\t age  =  pre_presbyopic :\n",
      "\t\t\t\t prediction:  none\n",
      "\t\t\t age  =  presbyopic :\n",
      "\t\t\t\t prediction:  none\n",
      "\t\t\t age  =  young :\n",
      "\t\t\t\t prediction:  hard\n",
      "\t\t spectacle_prescrip  =  myope :\n",
      "\t\t\t prediction:  hard\n",
      " tear_prod_rate  =  reduced :\n",
      "\t prediction:  none\n",
      "\n",
      "TEST SCORE\n",
      "0.3333333333333333\n",
      "SPLIT INFORMATION GAINS\n",
      "[0.5487949406953985, 0.7704260414863778, 0.3166890883150208, 1.0, 0.4591479170272448, 0.9182958340544896]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load debug training and testing data \n",
    "debug_train_data = arff.loadarff('datasets/lenses.arff')\n",
    "debug_trainDF = pd.DataFrame(debug_train_data[0])\n",
    "for column in debug_trainDF.columns:\n",
    "    debug_trainDF[column] = debug_trainDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "debug_test_data = arff.loadarff('datasets/lenses_test.arff')\n",
    "debug_testDF = pd.DataFrame(debug_test_data[0])\n",
    "for column in debug_testDF.columns:\n",
    "    debug_testDF[column] = debug_testDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "# display(debug_trainDF.head(2))\n",
    "# display(debug_testDF.head(2))\n",
    "\n",
    "# Encode data\n",
    "debug_enc = OrdinalEncoder()\n",
    "debug_enc.fit(debug_trainDF)\n",
    "debug_train = debug_enc.transform(debug_trainDF)\n",
    "debug_test = debug_enc.transform(debug_testDF)\n",
    "\n",
    "# pp.pprint(debug_enc.categories_)\n",
    "# print_encodings(debug_enc)\n",
    "\n",
    "# Save metadata\n",
    "counts = debug_trainDF.nunique().to_numpy()[:-1]\n",
    "features = debug_trainDF.columns\n",
    "\n",
    "# X and Y splits\n",
    "X_train = debug_train[:, :-1]\n",
    "X_test = debug_test[:, :-1]\n",
    "Y_train = debug_train[:, -1]\n",
    "Y_test = debug_test[:, -1]\n",
    "\n",
    "# Train Decision Tree\n",
    "dtc = DTClassifier(counts=counts)\n",
    "dtc.fit(X_train, Y_train)\n",
    "\n",
    "# Print the decision tree\n",
    "print('DECISION TREE')\n",
    "dtc.print_tree(features, debug_enc)\n",
    "print()\n",
    "\n",
    "# Predict and compute model accuracy\n",
    "print('TEST SCORE')\n",
    "print(dtc.score(X_test, Y_test))\n",
    "\n",
    "# Print the information gain of every split you make.\n",
    "print('SPLIT INFORMATION GAINS')\n",
    "print(dtc.split_information_gains)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " outlook  =  overcast :\n",
      "\t prediction:  yes\n",
      " outlook  =  rain :\n",
      "\t wind  =  strong :\n",
      "\t\t prediction:  no\n",
      "\t wind  =  weak :\n",
      "\t\t prediction:  yes\n",
      " outlook  =  sunny :\n",
      "\t humidity  =  high :\n",
      "\t\t prediction:  no\n",
      "\t humidity  =  normal :\n",
      "\t\t prediction:  yes\n"
     ]
    }
   ],
   "source": [
    "# TENNIS EXAMPLE\n",
    "# Load debug training and testing data\n",
    "example_train_data = arff.loadarff('datasets/examples/tennis.arff')\n",
    "example_trainDF = pd.DataFrame(example_train_data[0])\n",
    "for column in example_trainDF.columns:\n",
    "    example_trainDF[column] = example_trainDF[column] \\\n",
    "                                    .astype(str).str \\\n",
    "                                    .split(\"\\'\", expand=True) \\\n",
    "                                    .iloc[:,1]\n",
    "\n",
    "# Encode data\n",
    "example_enc = OrdinalEncoder()\n",
    "example_enc.fit(example_trainDF)\n",
    "example_train = example_enc.transform(example_trainDF)\n",
    "example_test = example_enc.transform(example_trainDF)\n",
    "\n",
    "# pp.pprint(example_enc.categories_)\n",
    "# print_encodings(example_enc)\n",
    "\n",
    "# Save metadata\n",
    "counts = example_trainDF.nunique().to_numpy()[:-1]\n",
    "features = example_trainDF.columns\n",
    "\n",
    "# X and Y splits\n",
    "X_train_example = example_train[:, :-1]\n",
    "Y_train_example = example_train[:, -1]\n",
    "\n",
    "# Train Decision Tree\n",
    "dtc = DTClassifier(counts=counts)\n",
    "dtc.fit(X_train_example, Y_train_example)\n",
    "dtc.print_tree(features, example_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional/Additional Debugging Dataset - Pizza Homework\n",
    "# pizza_dataset = np.array([[1,2,0],[0,0,0],[0,1,1],[1,1,1],[1,0,0],[1,0,1],[0,2,1],[1,0,0],[0,2,0]])\n",
    "# pizza_labels = np.array([2,0,1,2,1,2,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluation\n",
    "\n",
    "We will evaluate your model based on its performance on the zoo dataset. \n",
    "\n",
    "Train your model using this dataset: [Evaluation Train Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo.arff)\n",
    "\n",
    "Test your model on this dataset: [Evaluation Test Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/zoo_test.arff)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "Print out your accuracy on the evaluation test dataset.\n",
    "\n",
    "Print out the information gain of every split you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  hair feathers eggs milk airborne predator aquatic toothed backbone breathes  \\\n0    T        F    F    T        F        F       T       T        T        T   \n1    T        F    F    T        F        F       F       T        T        T   \n2    F        F    T    F        F        T       T       T        T        F   \n3    T        F    F    T        F        F       T       T        T        T   \n4    T        F    F    T        F        F       T       T        T        T   \n\n  venomous fins legs tails domestic catsize type  \n0        F    F    4     F        F       T   cT  \n1        F    F    4     T        F       T   cT  \n2        F    T    0     T        F       F   c4  \n3        F    F    4     F        F       T   cT  \n4        F    F    4     T        F       T   cT  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hair</th>\n      <th>feathers</th>\n      <th>eggs</th>\n      <th>milk</th>\n      <th>airborne</th>\n      <th>predator</th>\n      <th>aquatic</th>\n      <th>toothed</th>\n      <th>backbone</th>\n      <th>breathes</th>\n      <th>venomous</th>\n      <th>fins</th>\n      <th>legs</th>\n      <th>tails</th>\n      <th>domestic</th>\n      <th>catsize</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>4</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>cT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>4</td>\n      <td>T</td>\n      <td>F</td>\n      <td>T</td>\n      <td>cT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>0</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>c4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>4</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>cT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n      <td>4</td>\n      <td>T</td>\n      <td>F</td>\n      <td>T</td>\n      <td>cT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      " legs  =  0 :\n",
      "\t fins  =  F :\n",
      "\t\t toothed  =  F :\n",
      "\t\t\t prediction:  c7\n",
      "\t\t toothed  =  T :\n",
      "\t\t\t prediction:  c3\n",
      "\t fins  =  T :\n",
      "\t\t eggs  =  F :\n",
      "\t\t\t prediction:  cT\n",
      "\t\t eggs  =  T :\n",
      "\t\t\t prediction:  c4\n",
      " legs  =  2 :\n",
      "\t hair  =  F :\n",
      "\t\t prediction:  c2\n",
      "\t hair  =  T :\n",
      "\t\t prediction:  cT\n",
      " legs  =  4 :\n",
      "\t hair  =  F :\n",
      "\t\t predator  =  F :\n",
      "\t\t\t prediction:  c3\n",
      "\t\t predator  =  T :\n",
      "\t\t\t toothed  =  F :\n",
      "\t\t\t\t prediction:  c7\n",
      "\t\t\t toothed  =  T :\n",
      "\t\t\t\t prediction:  c5\n",
      "\t hair  =  T :\n",
      "\t\t prediction:  cT\n",
      " legs  =  5 :\n",
      "\t prediction:  c7\n",
      " legs  =  6 :\n",
      "\t predator  =  F :\n",
      "\t\t prediction:  c6\n",
      "\t predator  =  T :\n",
      "\t\t prediction:  c7\n",
      " legs  =  8 :\n",
      "\t prediction:  c7\n",
      "\n",
      "TEST SCORE\n",
      "0.147\n",
      "SPLIT INFORMATION GAINS\n",
      "['1.363', '0.887', '0.985', '0.696', '0.826', '0.689', '0.863', '0.722', '0.722']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation training data\n",
    "eval_train_data = arff.loadarff('datasets/zoo.arff')\n",
    "eval_trainDF = pd.DataFrame(eval_train_data[0])\n",
    "for column in eval_trainDF.columns:\n",
    "    eval_trainDF[column] = eval_trainDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "display(eval_trainDF.head())\n",
    "\n",
    "# Load evaluation test data\n",
    "eval_test_data = arff.loadarff('datasets/zoo_test.arff')\n",
    "eval_testDF = pd.DataFrame(eval_test_data[0])\n",
    "for column in eval_testDF.columns:\n",
    "    eval_testDF[column] = eval_testDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "# display(debug_trainDF.head(2))\n",
    "# display(debug_testDF.head(2))\n",
    "\n",
    "# Encode data\n",
    "eval_enc = OrdinalEncoder()\n",
    "eval_enc.fit(eval_trainDF)\n",
    "eval_train = eval_enc.transform(eval_trainDF)\n",
    "eval_test = eval_enc.transform(eval_testDF)\n",
    "\n",
    "# Save metadata\n",
    "eval_counts = eval_trainDF.nunique().to_numpy()[:-1]\n",
    "eval_features = eval_trainDF.columns\n",
    "\n",
    "# X and Y splits\n",
    "X_eval_train = eval_train[:, :-1]\n",
    "X_eval_test = eval_test[:, :-1]\n",
    "Y_eval_train = eval_train[:, -1]\n",
    "Y_eval_test = eval_test[:, -1]\n",
    "\n",
    "# Train Decision Tree\n",
    "dtc = DTClassifier(counts=eval_counts)\n",
    "dtc.fit(X_eval_train, Y_eval_train)\n",
    "\n",
    "# Print the decision tree\n",
    "print('DECISION TREE')\n",
    "dtc.print_tree(eval_features, eval_enc)\n",
    "print()\n",
    "\n",
    "# Predict and compute model accuracy\n",
    "print('TEST SCORE')\n",
    "print(dtc.score(X_eval_test, Y_eval_test))\n",
    "\n",
    "# Print out the information gain for every split you make\n",
    "print('SPLIT INFORMATION GAINS')\n",
    "print ([\"%0.3f\" % i for i in dtc.split_information_gains])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criteria, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
    "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
    "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
    "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that implements 10-fold cross validation\n",
    "def ten_fold_cv(X, Y, counts, max_depth=np.inf):\n",
    "    sum = 0\n",
    "    len_test_fold = X.shape[0] // 10\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        test_fold_start_ind = i * len_test_fold\n",
    "        test_fold_end_ind = test_fold_start_ind + len_test_fold\n",
    "        indices = np.array([test_fold_start_ind, test_fold_end_ind])\n",
    "\n",
    "        X_arrays = np.split(X, indices, axis=0)\n",
    "        Y_arrays = np.split(Y, indices, axis=0)\n",
    "        X_train = np.append(X_arrays[0], X_arrays[2], axis=0)\n",
    "        Y_train = np.append(Y_arrays[0], Y_arrays[2], axis=0)\n",
    "        X_test = X_arrays[1]\n",
    "        Y_test = Y_arrays[1]\n",
    "\n",
    "        dtc = DTClassifier(counts=counts, max_depth=max_depth)\n",
    "        dtc.fit(X_train, Y_train)\n",
    "        train_score = dtc.score(X_train, Y_train)\n",
    "        test_score = dtc.score(X_test, Y_test)\n",
    "\n",
    "        print(\"FOLD %d\" % (i + 1))\n",
    "        print(\"Train score: %f, test score: %f\" % (train_score, test_score))\n",
    "\n",
    "        sum += test_score\n",
    "\n",
    "    avg = sum / 10\n",
    "    print(\"Average test score: %f\" % avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Cars Dataset\n",
    "- Use this [Cars Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/cars.arff)\n",
    "- Make a table for your K-Fold cross validation accuracies\n",
    "\n",
    "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
    "\n",
    "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
    "\n",
    "*import arff as arf*                   \n",
    "\n",
    "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
    "\n",
    "*df = pd.DataFrame(cars)*  \n",
    "\n",
    "*There may be additional cleaning needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  buying  maint doors persons lug_boot safety  class\n0  vhigh  vhigh     2       2    small    low  unacc\n1  vhigh  vhigh     2       2    small    med  unacc\n2  vhigh  vhigh     2       2    small   high  unacc\n3  vhigh  vhigh     2       2      med    low  unacc\n4  vhigh  vhigh     2       2      med    med  unacc",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>buying</th>\n      <th>maint</th>\n      <th>doors</th>\n      <th>persons</th>\n      <th>lug_boot</th>\n      <th>safety</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vhigh</td>\n      <td>vhigh</td>\n      <td>2</td>\n      <td>2</td>\n      <td>small</td>\n      <td>low</td>\n      <td>unacc</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>vhigh</td>\n      <td>vhigh</td>\n      <td>2</td>\n      <td>2</td>\n      <td>small</td>\n      <td>med</td>\n      <td>unacc</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vhigh</td>\n      <td>vhigh</td>\n      <td>2</td>\n      <td>2</td>\n      <td>small</td>\n      <td>high</td>\n      <td>unacc</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>vhigh</td>\n      <td>vhigh</td>\n      <td>2</td>\n      <td>2</td>\n      <td>med</td>\n      <td>low</td>\n      <td>unacc</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vhigh</td>\n      <td>vhigh</td>\n      <td>2</td>\n      <td>2</td>\n      <td>med</td>\n      <td>med</td>\n      <td>unacc</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Train score: 0.984576, test score: 0.697674\n",
      "FOLD 2\n",
      "Train score: 0.974936, test score: 0.779070\n",
      "FOLD 3\n",
      "Train score: 0.992931, test score: 0.866279\n",
      "FOLD 4\n",
      "Train score: 0.980720, test score: 0.918605\n",
      "FOLD 5\n",
      "Train score: 0.980077, test score: 0.848837\n",
      "FOLD 6\n",
      "Train score: 0.965938, test score: 0.790698\n",
      "FOLD 7\n",
      "Train score: 0.964010, test score: 0.877907\n",
      "FOLD 8\n",
      "Train score: 0.989075, test score: 0.738372\n",
      "FOLD 9\n",
      "Train score: 0.960154, test score: 0.779070\n",
      "FOLD 10\n",
      "Train score: 0.989717, test score: 0.720930\n",
      "Average test score: 0.801744\n",
      "\n",
      "Score on all data: 0.8993055555555556\n",
      " safety  =  high :\n",
      "\t persons  =  2 :\n",
      "\t\t prediction:  unacc\n",
      "\t persons  =  4 :\n",
      "\t\t buying  =  high :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t buying  =  low :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  med :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  vhigh :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t persons  =  more :\n",
      "\t\t buying  =  high :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t buying  =  low :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  vgood\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  vgood\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  vgood\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  med :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  vgood\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  vgood\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  vhigh :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n",
      " safety  =  low :\n",
      "\t prediction:  unacc\n",
      " safety  =  med :\n",
      "\t persons  =  2 :\n",
      "\t\t prediction:  unacc\n",
      "\t persons  =  4 :\n",
      "\t\t buying  =  high :\n",
      "\t\t\t lug_boot  =  big :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t lug_boot  =  med :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t\t lug_boot  =  small :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t buying  =  low :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  med :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  vhigh :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t persons  =  more :\n",
      "\t\t buying  =  high :\n",
      "\t\t\t lug_boot  =  big :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t lug_boot  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t lug_boot  =  small :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t buying  =  low :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  med :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  good\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t buying  =  vhigh :\n",
      "\t\t\t maint  =  high :\n",
      "\t\t\t\t prediction:  unacc\n",
      "\t\t\t maint  =  low :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  med :\n",
      "\t\t\t\t prediction:  acc\n",
      "\t\t\t maint  =  vhigh :\n",
      "\t\t\t\t prediction:  unacc\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold CV on Cars Dataset\n",
    "# Load evaluation training data\n",
    "cars_train_data = arff.loadarff('datasets/cars.arff')\n",
    "cars_trainDF = pd.DataFrame(cars_train_data[0])\n",
    "for column in cars_trainDF.columns:\n",
    "    cars_trainDF[column] = cars_trainDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "display(cars_trainDF.head())\n",
    "\n",
    "# Encode data\n",
    "cars_enc = OrdinalEncoder()\n",
    "cars_enc.fit(cars_trainDF)\n",
    "cars_train = cars_enc.transform(cars_trainDF)\n",
    "\n",
    "# Save metadata\n",
    "cars_counts = cars_trainDF.nunique().to_numpy()[:-1]\n",
    "cars_features = cars_trainDF.columns\n",
    "\n",
    "# X and Y splits\n",
    "X_cars_train = cars_train[:, :-1]\n",
    "Y_cars_train = cars_train[:, -1]\n",
    "\n",
    "ten_fold_cv(X_cars_train, Y_cars_train, cars_counts, 10)\n",
    "\n",
    "print()\n",
    "\n",
    "dtc = DTClassifier(counts=cars_counts, max_depth=5)\n",
    "print(\"Score on all data: {}\".format(dtc.fit(X_cars_train, Y_cars_train).score(X_cars_train, Y_cars_train)))\n",
    "dtc.print_tree(cars_features, cars_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Voting Dataset\n",
    "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)\n",
    "- Note that you will need to support unknown attributes in the voting data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Train score: 0.971939, test score: 0.976744\n",
      "FOLD 2\n",
      "Train score: 0.966837, test score: 0.953488\n",
      "FOLD 3\n",
      "Train score: 0.969388, test score: 1.000000\n",
      "FOLD 4\n",
      "Train score: 0.966837, test score: 0.953488\n",
      "FOLD 5\n",
      "Train score: 0.969388, test score: 0.976744\n",
      "FOLD 6\n",
      "Train score: 0.964286, test score: 0.930233\n",
      "FOLD 7\n",
      "Train score: 0.969388, test score: 0.976744\n",
      "FOLD 8\n",
      "Train score: 0.979592, test score: 0.953488\n",
      "FOLD 9\n",
      "Train score: 0.984694, test score: 0.906977\n",
      "FOLD 10\n",
      "Train score: 0.974490, test score: 0.930233\n",
      "Average test score: 0.955814\n",
      "\n",
      "Score on all data: 0.9724137931034482\n",
      " physician-fee-freeze  =  UNKNOWN :\n",
      "\t mx-missile  =  UNKNOWN :\n",
      "\t\t prediction:  republican\n",
      "\t mx-missile  =  n :\n",
      "\t\t prediction:  democrat\n",
      "\t mx-missile  =  y :\n",
      "\t\t anti-satellite-test-ban  =  UNKNOWN :\n",
      "\t\t\t prediction:  democrat\n",
      "\t\t anti-satellite-test-ban  =  n :\n",
      "\t\t\t prediction:  republican\n",
      "\t\t anti-satellite-test-ban  =  y :\n",
      "\t\t\t prediction:  democrat\n",
      " physician-fee-freeze  =  n :\n",
      "\t adoption-of-the-budget-resolution  =  UNKNOWN :\n",
      "\t\t prediction:  democrat\n",
      "\t adoption-of-the-budget-resolution  =  n :\n",
      "\t\t education-spending  =  UNKNOWN :\n",
      "\t\t\t prediction:  republican\n",
      "\t\t education-spending  =  n :\n",
      "\t\t\t prediction:  democrat\n",
      "\t\t education-spending  =  y :\n",
      "\t\t\t prediction:  democrat\n",
      "\t adoption-of-the-budget-resolution  =  y :\n",
      "\t\t prediction:  democrat\n",
      " physician-fee-freeze  =  y :\n",
      "\t synfuels-corporation-cutback  =  UNKNOWN :\n",
      "\t\t prediction:  republican\n",
      "\t synfuels-corporation-cutback  =  n :\n",
      "\t\t duty-free-exports  =  UNKNOWN :\n",
      "\t\t\t prediction:  republican\n",
      "\t\t duty-free-exports  =  n :\n",
      "\t\t\t prediction:  republican\n",
      "\t\t duty-free-exports  =  y :\n",
      "\t\t\t prediction:  republican\n",
      "\t synfuels-corporation-cutback  =  y :\n",
      "\t\t adoption-of-the-budget-resolution  =  UNKNOWN :\n",
      "\t\t\t prediction:  democrat\n",
      "\t\t adoption-of-the-budget-resolution  =  n :\n",
      "\t\t\t prediction:  republican\n",
      "\t\t adoption-of-the-budget-resolution  =  y :\n",
      "\t\t\t prediction:  democrat\n"
     ]
    }
   ],
   "source": [
    "# Load vote data \n",
    "vote_data = arff.loadarff('datasets/voting_with_missing.arff')\n",
    "voteDF = pd.DataFrame(vote_data[0])\n",
    "for column in voteDF.columns:\n",
    "    voteDF[column] = voteDF[column] \\\n",
    "                         .astype(str).str \\\n",
    "                         .split(\"\\'\", expand=True) \\\n",
    "                         .iloc[:,1]\n",
    "\n",
    "m = lambda x: \"UNKNOWN\" if x == \"?\" else x\n",
    "voteDF = voteDF.applymap(m)\n",
    "\n",
    "# Encode data\n",
    "enc_vote = OrdinalEncoder()\n",
    "enc_vote.fit(voteDF)\n",
    "vote = enc_vote.transform(voteDF)\n",
    "\n",
    "# Save metadata\n",
    "vote_counts = voteDF.nunique().to_numpy()[:-1]\n",
    "vote_features = voteDF.columns\n",
    "\n",
    "X_vote_train = vote[:, :-1]\n",
    "Y_vote_train = vote[:, -1]\n",
    "\n",
    "ten_fold_cv(X_vote_train, Y_vote_train, vote_counts, max_depth=10)\n",
    "\n",
    "print()\n",
    "\n",
    "dtc = DTClassifier(counts=vote_counts)\n",
    "print(\"Score on all data: {}\".format(dtc.fit(X_vote_train, Y_vote_train).score(X_vote_train, Y_vote_train)))\n",
    "dtc.print_tree(vote_features, enc_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Discuss Your Results\n",
    "\n",
    "- Summarize your results from both datasets, and discuss what you observed. \n",
    "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full decision tree for the cars dataset achieved a mean test accuracy of 80.17% in the 10-fold cross validation. This seems lower than the \"sanity-check\" score of 90-95% accuracy. The full decision tree for the vote dataset achieved a mean test accuracy of 95.58% in the 10-fold cross validation. This seems in check with the guideline accuracy of 92-95%. For either data set, the trees became very deep.\n",
    "\n",
    "A fully expanded tree will often get 100% accuracy on the training set because the decision tree will learn to make splits so that every data instance is classified correctly. If there is a case where two or more instances have the same feature values but different output classes, then a fully expanded tree will not get 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e. look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
    "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cars dataset, the tree has learned what features in-order are most important in predicting car acceptability. It seems that car safety is the most important factor, followed by carrying capacity, and then by buying price, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Discuss what the decision tree induced on the voting dataset has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vote dataset, the tree has learned what political issues, in order, are most influential in predicting a representative's political party. In other words, it has sorted political issues in order from most partisan to most bipartisan. The best issue to split on heavily depends on the previous issue split on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put unknown attributes in the class \"UNKNOWN\". As per my preference, I didn't want to have to code up an algorithm to find the most similar instance for imputing the unknown attribute. However, given more time on the project, this would make sense because politicians who vote similarly on other issues are likely to vote the same on a new issue. Further, just because we don't know what a politician voted for does not mean they didn't vote. Therefore, they shouldn't be excluded from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 (10%) Use SciKit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform the best on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 SK Learn on Voting Dataset\n",
    "- Use this [Voting Dataset with missing values](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/voting_with_missing.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SKLearn DTC vote dataset test set accuracy: 0.943\n",
      "SKLearn DTC w/ max_depth=5 vote dataset train set accuracy: 0.954\n",
      "SKLearn DTC w/ min_samples_split=5 vote dataset train set accuracy: 0.920\n",
      "SKLearn DTC w/ min_samples_leaf=3 vote dataset train set accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_vote_train, Y_vote_train, test_size=0.2, random_state=2)\n",
    "\n",
    "# figure(figsize=(40, 40), dpi=300)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Default SKLearn DTC vote dataset test set accuracy: %.3f\" % (clf.score(X_test, Y_test)))\n",
    "\n",
    "# tree.plot_tree(clf)\n",
    "# plt.show()\n",
    "\n",
    "# Explore different parameters\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"SKLearn DTC w/ max_depth=5 vote dataset train set accuracy: %.3f\" % (clf.score(X_test, Y_test)))\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, min_samples_split=5)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"SKLearn DTC w/ min_samples_split=5 vote dataset train set accuracy: %.3f\" % (clf.score(X_test, Y_test)))\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, min_samples_leaf=3)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"SKLearn DTC w/ min_samples_leaf=3 vote dataset train set accuracy: %.3f\" % (clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss results & compare to your method's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the SK decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  gender             ssc_p    ssc_b             hsc_p    hsc_b     hsc_s  \\\n0      M  (62.467, 75.933]   Others    (82.077, 97.7]   Others  Commerce   \n1      M    (75.933, 89.4]  Central  (66.453, 82.077]   Others   Science   \n2      M  (62.467, 75.933]  Central  (66.453, 82.077]  Central      Arts   \n\n           degree_p   degree_t workex         etest_p specialisation  \\\n0  (55.965, 67.667]   Sci&Tech     No  (49.952, 66.0]         Mkt&HR   \n1  (67.667, 79.333]   Sci&Tech    Yes    (82.0, 98.0]        Mkt&Fin   \n2  (55.965, 67.667]  Comm&Mgmt     No    (66.0, 82.0]        Mkt&Fin   \n\n              mba_p  status                  salary  \n0  (52.354, 60.883]  Placed  (199260.0, 305714.286]  \n1  (60.883, 69.387]  Placed  (199260.0, 305714.286]  \n2  (52.354, 60.883]  Placed  (199260.0, 305714.286]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gender</th>\n      <th>ssc_p</th>\n      <th>ssc_b</th>\n      <th>hsc_p</th>\n      <th>hsc_b</th>\n      <th>hsc_s</th>\n      <th>degree_p</th>\n      <th>degree_t</th>\n      <th>workex</th>\n      <th>etest_p</th>\n      <th>specialisation</th>\n      <th>mba_p</th>\n      <th>status</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>M</td>\n      <td>(62.467, 75.933]</td>\n      <td>Others</td>\n      <td>(82.077, 97.7]</td>\n      <td>Others</td>\n      <td>Commerce</td>\n      <td>(55.965, 67.667]</td>\n      <td>Sci&amp;Tech</td>\n      <td>No</td>\n      <td>(49.952, 66.0]</td>\n      <td>Mkt&amp;HR</td>\n      <td>(52.354, 60.883]</td>\n      <td>Placed</td>\n      <td>(199260.0, 305714.286]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>(75.933, 89.4]</td>\n      <td>Central</td>\n      <td>(66.453, 82.077]</td>\n      <td>Others</td>\n      <td>Science</td>\n      <td>(67.667, 79.333]</td>\n      <td>Sci&amp;Tech</td>\n      <td>Yes</td>\n      <td>(82.0, 98.0]</td>\n      <td>Mkt&amp;Fin</td>\n      <td>(60.883, 69.387]</td>\n      <td>Placed</td>\n      <td>(199260.0, 305714.286]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>(62.467, 75.933]</td>\n      <td>Central</td>\n      <td>(66.453, 82.077]</td>\n      <td>Central</td>\n      <td>Arts</td>\n      <td>(55.965, 67.667]</td>\n      <td>Comm&amp;Mgmt</td>\n      <td>No</td>\n      <td>(66.0, 82.0]</td>\n      <td>Mkt&amp;Fin</td>\n      <td>(52.354, 60.883]</td>\n      <td>Placed</td>\n      <td>(199260.0, 305714.286]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SKLearn DTC defaults vote dataset test set accuracy: 0.6666666666666666\n",
      "Default SKLearn DTC max depth 4 vote dataset test set accuracy: 0.8\n",
      "Default SKLearn DTC min samples split 15 vote dataset test set accuracy: 0.7\n",
      "Default SKLearn DTC min samples leaf 8 vote dataset test set accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Use SciKit Learn's Decision Tree on a new dataset\n",
    "df = pd.read_csv(\"datasets/csv/Placement_Data_Full_Class.csv\")\n",
    "\n",
    "df.dropna(inplace=True) # I know this usually isn't good but it's easier\n",
    "df['ssc_p'] = pd.cut(df['ssc_p'], 3)\n",
    "df['hsc_p'] = pd.cut(df['hsc_p'], 3)\n",
    "df['degree_p'] = pd.cut(df['degree_p'], 3)\n",
    "df['etest_p'] = pd.cut(df['etest_p'], 3)\n",
    "df['mba_p'] = pd.cut(df['mba_p'], 3)\n",
    "df['salary'] = pd.cut(df['salary'], 7)\n",
    "df.drop(columns=['sl_no'], inplace=True)\n",
    "display(df.head(3))\n",
    "\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].astype(str)\n",
    "\n",
    "# Encode data\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(df)\n",
    "data = enc.transform(df)\n",
    "categories = enc.categories_[-1]\n",
    "\n",
    "# Save metadata\n",
    "counts = df.nunique().to_numpy()[:-1]\n",
    "feature_names = df.columns[:-1]\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Experiment with different hyper-parameters\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(\"Default SKLearn DTC defaults vote dataset test set accuracy: {}\".format(clf.score(x_test, y_test)))\n",
    "\n",
    "clf_depth = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "clf_depth.fit(x_train, y_train)\n",
    "\n",
    "print(\"Default SKLearn DTC max depth 4 vote dataset test set accuracy: {}\".format(clf_depth.score(x_test, y_test)))\n",
    "\n",
    "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=15, random_state=0)\n",
    "clf_min_samples_split.fit(x_train, y_train)\n",
    "\n",
    "print(\"Default SKLearn DTC min samples split 15 vote dataset test set accuracy: {}\".format(clf_min_samples_split.score(x_test, y_test)))\n",
    "\n",
    "clf_min_samples_leaf = DecisionTreeClassifier(min_samples_leaf=8, random_state=0)\n",
    "clf_min_samples_leaf.fit(x_train, y_train)\n",
    "\n",
    "print(\"Default SKLearn DTC min samples leaf 8 vote dataset test set accuracy: {}\".format(clf_min_samples_leaf.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first several levels (e.g. top 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 6000x6000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Include decision tree visualization here\n",
    "figure(figsize=(20,20), dpi=300)\n",
    "# tree.plot_tree(clf_depth, feature_names=feature_names, class_names=enc.categories_[-1])\n",
    "tree.export_graphviz(clf, out_file=\"tree.dot\", max_depth=2, label='root',\n",
    "                     feature_names=feature_names, impurity=False, rounded=True)\n",
    "plt.show()\n",
    "\n",
    "# Discuss what the model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SciKit-Learn DecisionTreeClassifier has learned from my MBA dataset that most of these graduated students have salaries between ~$200,000 and ~$300,000 and predicts that class most of the time. This of course is affected greatly by how I have binned my continuous features in the dataset. However, having more bins leads to much lower accuracy. It turns out that making a prediction in this range almost all the time leads to the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
    "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
    "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
    "- This table should compare:\n",
    "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
    "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b2efc4b102cc6878f14c71b1b299607eba1e6d0bb881d7beb2f5cad1213cf03"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}