{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR"
   },
   "source": [
    "# K-Nearest Neighbor Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6ZbYjZZZ_yLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import arff\n",
    "from IPython.core.display import display\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj"
   },
   "source": [
    "## 1. (40%) Correctly implement the k-nearest neighbor (KNN) algorithm and the KNN regression algorithm\n",
    "\n",
    "### Code requirements\n",
    "- Use Euclidean distance to decide closest neighbors. \n",
    "- Include optional distance weighting for both algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_a2KSZ_7AN0G"
   },
   "outputs": [],
   "source": [
    "class KNNClassifier(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self, columntype=[], weight_type='inverse_distance', knn=3): ## add parameters here\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            columntype for each column tells you if continues[real] or if nominal[categoritcal].\n",
    "            weight_type: inverse_distance voting or if non distance weighting. Options = [\"no_weight\",\"inverse_distance\"]\n",
    "        \"\"\"\n",
    "        self.columntype = columntype #Note This won't be needed until part 5\n",
    "        self.weight_type = weight_type\n",
    "        self.knn = knn\n",
    "\n",
    "    def fit(self, data, labels):\n",
    "        \"\"\" Fit the data; run the algorithm (for this lab really just saves the data :D)\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "            y (array-like): A 2D numpy array with the training targets\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\" Predict all classes for a dataset X\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    #Returns the Mean score given input data and labels\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 2D numpy array with targets\n",
    "        Returns:\n",
    "            score : float\n",
    "                Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Debug and Evaluation\n",
    "\n",
    "Debug and Evaluate your model using the parameters below:\n",
    "\n",
    "- Use distance weighting\n",
    "- KNN = 3 (three nearest neighbors)\n",
    "- Don’t normalize the data\n",
    "- Use Euclidean Distance\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1.1 Debug\n",
    "\n",
    "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/glass_test.arff)\n",
    "- Use distance weighting\n",
    "- KNN = 3 (three nearest neighbors)\n",
    "- Don’t normalize the data\n",
    "- Use Euclidean Distance\n",
    "\n",
    "Expected Results:\n",
    "- Not using inverse weighted distancing = roughly [68.29%]\n",
    "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_no_inv_prediction.csv)\n",
    "\n",
    "- Using inverse weighted distancing = roughly [74.39%]\n",
    "- Link to [debug solution](https://github.com/cs472ta/CS472/blob/master/debug_solutions/glass_inv_prediction.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>'K'</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.51793</td>\n",
       "      <td>12.79</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.12</td>\n",
       "      <td>73.03</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>build wind float</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51643</td>\n",
       "      <td>12.16</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.35</td>\n",
       "      <td>72.89</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vehic wind float</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si   'K'    Ca   Ba   Fe              Type\n",
       "0  1.51793  12.79  3.50  1.12  73.03  0.64  8.77  0.0  0.0  build wind float\n",
       "1  1.51643  12.16  3.52  1.35  72.89  0.57  8.53  0.0  0.0  vehic wind float"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI      float64\n",
      "Na      float64\n",
      "Mg      float64\n",
      "Al      float64\n",
      "Si      float64\n",
      "'K'     float64\n",
      "Ca      float64\n",
      "Ba      float64\n",
      "Fe      float64\n",
      "Type     object\n",
      "dtype: object\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "# Load glass data\n",
    "glass_train_data = arff.loadarff('datasets/glass_train.arff')\n",
    "glass_trainDF = pd.DataFrame(glass_train_data[0])\n",
    "glass_trainDF.iloc[:, -1] = glass_trainDF.iloc[:, -1] \\\n",
    "                        .astype(str).str \\\n",
    "                        .split(\"\\'\", expand=True) \\\n",
    "                        .iloc[:,1]\n",
    "\n",
    "display(glass_trainDF.head(2))\n",
    "\n",
    "# Get column types\n",
    "column_type = glass_trainDF.dtypes.map({'float64': 'continuous', 'object': 'nominal'}).tolist()\n",
    "print(type(glass_trainDF.dtypes)\n",
    "print(column_type)\n",
    "\n",
    "# Train on training set \n",
    "glass_knn = KNNClassifier()\n",
    "\n",
    "# Predict on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Evaluate\n",
    "\n",
    "We will evaluate your model based on its performance on the [diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes) problem.\n",
    "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/diabetes_test.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes data\n",
    "\n",
    "# Train on training set\n",
    "\n",
    "# Predict on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh"
   },
   "source": [
    "## 2. (10%) Use the k-nearest neighbor algorithm (without distance weighting) for the [magic telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) problem\n",
    "\n",
    "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/magic_telescope_test.arff) \n",
    "\n",
    "### 2.1\n",
    "- Try it with k=3 and without normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4SSoasDQSKXb"
   },
   "outputs": [],
   "source": [
    "# Load magic telescope data\n",
    "\n",
    "# Train/Predict without normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "- Try it with k=3 and with normalization (input features normalized between 0 and 1). Use the normalization formula (x-xmin)/(xmax-xmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Predict with normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss the accuracy results of using normalized data vs. unnormalized data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "- Using your normalized data, create one graph with classification accuracy on the test set over k values. \n",
    "    - Use odd values of k from 1 to 15.\n",
    "- As a rough sanity check, typical knn accuracies for the magic telescope data set are 75-85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Predict with normalization using k=1,3,...,15\n",
    "\n",
    "# Graph classification accuracy over k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the rest of the experiments use only normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x"
   },
   "source": [
    "## 3. (10%) Use the regression variation of your algorithm (without distance weighting) for the [housing price prediction](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) problem.\n",
    "\n",
    "- Use this [training set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_train.arff) and this [test set](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/housing_test.arff).\n",
    "- Use Mean Square Error (MSE) on the test set as your accuracy metric for this case.\n",
    "    - Do not normalize regression output values\n",
    "- Graph MSE on the test set with odd values of k from 1 to 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KBGUn43ASiXW"
   },
   "outputs": [],
   "source": [
    "# Load housing price prediction data\n",
    "\n",
    "# Train/Predict using k=1,3,...,15\n",
    "\n",
    "# Graph MSE over k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v19fpixqTe-7"
   },
   "source": [
    "## 4. (15%) Repeat your experiments for magic telescope and housing using distance-weighted (inverse of distance squared) voting and discuss your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Magic Telescope Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZCPFUAGTS2sX"
   },
   "outputs": [],
   "source": [
    "# Train/Predict magic telescope using distance-weighted voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Predict housing using distance-weighted voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (10%) Use the k-nearest neighbor algorithm to solve the [credit-approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval) (credit-a) problem.\n",
    "\n",
    "- Use this [dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/credit_approval.arff)\n",
    "    - Use a 70/30 split of the data for the training/test set\n",
    "- Note that this set has both continuous and nominal attributes, together with don’t know values. \n",
    "- Implement and justify a distance metric which supports continuous, nominal, and don’t know attribute values\n",
    "    - You need to handle don't knows with the distance metric, not by imputing a value.\n",
    "    - More information on distance metrics can be found [here](https://www.jair.org/index.php/jair/article/view/10182/24168).\n",
    "- Use your own choice for k.\n",
    "- As a rough sanity check, typical knn accuracies for the credit data set are 70-80%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into train/test sets\n",
    "\n",
    "# Train/Predict credit-approval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain and justify your distance metric*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "## 6. (15%) Use the scikit's KNN Classifier on magic telescope and KNN Regressor on housing and compare your results.\n",
    "\n",
    "- Try out different hyperparameters to see how well you can do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Train/Predict magic telescope using scikit's KNN\n",
    "\n",
    "# Train/Predict housing using scikit's KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms"
   },
   "source": [
    "*Report your comparison*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlK-kijk8Mg"
   },
   "source": [
    "## 7. (optional 5% extra credit): For the best value of k for any one of the datasets, implement a reduction algorithm that removes data points in some rational way such that performance does not drop too drastically on the test set given the reduced training set.\n",
    "\n",
    "- Compare your performance on the test set for the reduced and non-reduced versions and give the number (and percentage) of training examples removed from the original training set. How well does your reduction algorithm work?\n",
    "    - Note that performance for magic telescope is classification accuracy and for housing it is mean squared error.\n",
    "    - Magic Telescope has about 12,000 instances and if you use a leave one out style of testing for your data set reduction, then your algorithm will run slow since that is n2 at each step.\n",
    "    - If you wish, you may use a random subset of 2,000 of the magic telescope instances.\n",
    "    - More information on reduction techniques can be found [here](http://axon.cs.byu.edu/~martinez/classes/478/slides/IBL.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iY77P7gk1Nh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab 1 - perceptron",
   "provenance": []
  },
  "interpreter": {
   "hash": "6b2efc4b102cc6878f14c71b1b299607eba1e6d0bb881d7beb2f5cad1213cf03"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('general': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
